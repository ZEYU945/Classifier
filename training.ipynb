{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.load(\"Cas.npy\")\n",
    "label = label.reshape(4989,1)\n",
    "label = label - 1\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# train_img.shape\n",
    "# img = Image.fromarray(train_img[2], \"RGB\")\n",
    "# img.show()\n",
    "\n",
    "index = np.where(label==4)\n",
    "label = np.delete(label, np.where(label == 4))\n",
    "\n",
    "index_0 = np.where(label==0)\n",
    "index_0 = index_0[0][:3200]\n",
    "label = np.delete(label, index_0)\n",
    "# label = label.reshape(4956-3200, 1)\n",
    "\n",
    "index_1 = np.where(label==1)\n",
    "index_1 = index_1[0][:300]\n",
    "label = np.delete(label, index_1)\n",
    "label = label.reshape(4956-3200-300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with label 0 is (380,)\n",
      "The number of images with label 1 is (365,)\n",
      "The number of images with label 2 is (357,)\n",
      "The number of images with label 3 is (354,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"The number of images with label {i} is {np.where(label ==i)[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 2)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESIZED_ROOT = os.path.join(os.path.dirname(os.path.abspath('')),'extraction','files_resized_choose')\n",
    "path_list = os.listdir(RESIZED_ROOT)\n",
    "path_list.sort(key = lambda x:int(x.split('.')[0]))\n",
    "\n",
    "file = []\n",
    "for file_name in path_list:\n",
    "    file.append(file_name)\n",
    "\n",
    "file = np.array(file)\n",
    "file = np.delete(file, index)\n",
    "file = np.delete(file, index_0)\n",
    "file = np.delete(file, index_1)\n",
    "file = file.reshape(4956-3200-300,1)\n",
    "\n",
    "annotation = np.concatenate((file,label),axis=1)\n",
    "annotation.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = file\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((975, 2), (481, 2))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train\n",
    "annotation_train = np.concatenate((X_train,y_train),axis=1)\n",
    "annotation_test = np.concatenate((X_test,y_test),axis=1)\n",
    "(annotation_train.shape, annotation_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.DataFrame(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        label = int(label)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllSet = CustomImageDataset(annotation,\"files_resized_choose\")\n",
    "# AllSet.img_labels\n",
    "# image, label = AllSet.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(960), transforms.Grayscale()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(annotation_train,\"files_resized_choose\",transform=transform)\n",
    "test_data = CustomImageDataset(annotation_test,\"files_resized_choose\",transform=transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0][1].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(7, 7), stride=(3, 3))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(7, 7), stride=(3, 3))\n",
      "  (conv3): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 7, stride=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 7, stride=3)\n",
    "        self.conv3 = nn.Conv2d(16, 24, 5)\n",
    "        # self.conv4 = nn.Conv2d(24, 32, 5)\n",
    "        # self.conv5 = nn.Conv2d(32, 48, 3)\n",
    "        self.fc1 = nn.Linear(2400, 120)\n",
    "        self.fc2 = nn.Linear(120, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(F.relu(self.conv4(x)))\n",
    "        # x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.495\n",
      "[1,   200] loss: 1.388\n",
      "[2,   100] loss: 1.383\n",
      "[2,   200] loss: 1.383\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(torch.float)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # print(f'epoch:{epoch+1} loss:{running_loss / len(train_dataloader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images:28.307692307692307%\n",
      "Number of train images: 975\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_dataloader:\n",
    "        images = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        images = images.to(torch.float)\n",
    "        output = net(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the train images:{100 * correct / total}%')\n",
    "print('Number of train images:',total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: 0     is 98.4 %\n",
      "Accuracy for class: 1     is 3.2 %\n",
      "Accuracy for class: 2     is 0.0 %\n",
      "Accuracy for class: 3     is 1.6 %\n"
     ]
    }
   ],
   "source": [
    "classes =('0','1','2','3')\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        images = images.to(torch.float)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './classification.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
