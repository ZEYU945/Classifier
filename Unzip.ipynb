{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815e92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e89b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('files_resized_choose.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/workspace/Classifier/files_resized_choose')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3adae8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Classifier'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec97096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('files_choose.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/workspace/Classifier/files_choose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f880fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('validation.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/workspace/Classifier/CatDog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf46b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('CatPlusDog.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/workspace/Classifier/CatPlusDog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673bc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the mean and std of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e177e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ba00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12345->01234\n",
    "# delete 4, delete 1, 23->1\n",
    "label = np.load(\"Cas.npy\")\n",
    "label = label.reshape(4989,1)\n",
    "label = label - 1\n",
    "\n",
    "index = np.where(label==4)\n",
    "label = np.delete(label, np.where(label == 4))\n",
    "\n",
    "index_0 = np.where(label==0)\n",
    "index_0 = index_0[0][:2800]\n",
    "label = np.delete(label, index_0)\n",
    "\n",
    "index_1 = np.where(label==1)\n",
    "label = np.delete(label, index_1)\n",
    "\n",
    "label[label==2] = 1\n",
    "label[label==3] = 1\n",
    "\n",
    "label = label.reshape(1491, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3487deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with label 0 is (780,)\n",
      "The number of images with label 1 is (711,)\n",
      "The number of images with label 2 is (0,)\n",
      "The number of images with label 3 is (0,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"The number of images with label {i} is {np.where(label ==i)[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68794e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZED_ROOT = os.path.join(os.path.dirname(os.path.abspath('')),'Classifier','files_choose')\n",
    "path_list = os.listdir(RESIZED_ROOT)\n",
    "path_list.sort(key = lambda x:int(x.split('.')[0]))\n",
    "\n",
    "file = []\n",
    "for file_name in path_list:\n",
    "    file.append(file_name)\n",
    "\n",
    "file = np.array(file)\n",
    "file = np.delete(file, index)\n",
    "file = np.delete(file, index_0)\n",
    "file = np.delete(file, index_1)\n",
    "file = file.reshape(1491,1)\n",
    "\n",
    "annotation = np.concatenate((file,label),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66bd3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(annotation)\n",
    "# df = df.rename(columns={0: \"image_id\", 1: \"label\"})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "705d90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.DataFrame(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        image = image.to(torch.float)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "250bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((960,960),antialias=True),\n",
    "    transforms.Normalize((0,0,0),(1,1,1))])\n",
    "image_size = 960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b56f3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomImageDataset(annotation,\"files_choose\",transform=transform)\n",
    "dataloader = DataLoader(data, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "034855b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [00:16<00:00, 11.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs in tqdm(dataloader):\n",
    "    psum    += inputs.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c5f5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([145.0785, 136.7515, 126.1401])\n",
      "std:  tensor([52.9697, 52.6246, 55.6964])\n"
     ]
    }
   ],
   "source": [
    "# pixel count\n",
    "count = len(df) * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b25a30e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4653e+11, 1.4566e+11, 1.4352e+11])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psum_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e751eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9935e+11, 1.8791e+11, 1.7333e+11])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45bb225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([145.0785, 136.7515, 126.1402])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e21f68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([106.6331, 106.0066, 104.4488])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psum_sq / 1374105600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f344df42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374105600"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c52fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
